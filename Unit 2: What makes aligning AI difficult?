# AI Alignment Fast-Track

## What makes aligning AI difficult?

### How GPT-2 Became Maximally Lewd

* **Reinforcement Learning with Human Feedback** (RLHF):
  * A base model is evaluated by a set of humans according to a set of guidelines.
  * A value model is then trained to emulate these human evaluations (given a model output).
  * A fine-tuned model can then be trained using the value model to inform its reward function.
  * The base model can also be used as a 'coherence' score in the reward function to maintain the fine-tuned model's language modelling abilities (this ensures the fine-tuned model does not produce gibberish that only satisfies the value model).
  * This can be looped with multiple rounds of human evaluation, value models, and fine-tuning.
* 'Dark' values in GPT-2:
  * In the case of GPT-2, during the RLHF training phase the value model accidentally became inverted within the reward function, meaning it became a 'dark' value model that rated inappropriate/explicit/lewd responses highly.
  * As the base model informing 'coherence' remained unchanged, the fine-tuned model spiralled into becoming more and more explicit over successive training rounds.
  * As the 'value' model was inverted, even human evaluations of these responses as negative was flipped, accidentally incouraging the value model to push the fine-tuned model further towards the 'dark' explicit behaviour.
  * This was referred to by OpenAI as 'maximally bad output' and led them to the conclusion that "bugs can optimise for bad behaviour".

### RLAIF vs. RLHF: the technology behind Anthropicâ€™s Claude (Constitutional AI Explained)

* 
