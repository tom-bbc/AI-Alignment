# AI Alignment Fast-Track

## What makes aligning AI difficult?

### How GPT-2 Became Maximally Lewd

* **Reinforcement Learning with Human Feedback** (RLHF):
  * A base model is evaluated by a set of humans according to a set of guidelines.
  * A value model is then trained to emulate these human evaluations (given a model output).
  * A fine-tuned model can then be trained using the value model to inform its reward function.
  * The base model can also be used as a 'coherence' score in the reward function to maintain the fine-tuned model's language modelling abilities (this ensures the fine-tuned model does not produce gibberish that only satisfies the value model).
  * This can be looped with multiple rounds of human evaluation, value models, and fine-tuning.
* 'Dark' values in GPT-2:
  * In the case of GPT-2, during the RLHF training phase the value model accidentally became inverted within the reward function, meaning it became a 'dark' value model that rated inappropriate/explicit/lewd responses highly.
  * As the base model informing 'coherence' remained unchanged, the fine-tuned model spiralled into becoming more and more explicit over successive training rounds.
  * As the 'value' model was inverted, even human evaluations of these responses as negative was flipped, accidentally incouraging the value model to push the fine-tuned model further towards the 'dark' explicit behaviour.
  * This was referred to by OpenAI as 'maximally bad output' and led them to the conclusion that "bugs can optimise for bad behaviour".

### RLAIF vs. RLHF: the technology behind Anthropicâ€™s Claude (Constitutional AI Explained)

* Human feedback is not scalable.
* **Constitutional AI**:
 * Lays out a set of strict rules for an AI system.
 * The model must be helpful (non-evasive) and harmless.
 * This should allow the model to explain why it is not answering specific prompts (e.g. those requesting harmful behaviour).
* Training Constitutional AI:
 * Supervised learning phase uses harmful prompts and an existing RLHF trained model. 
 * The model first responds to the harmful prompt, then is asked to critique its own answer (w.r.t a constitutional principle), then use this critigue to regenerate a revised response.
 * The harmful prompts and revised responses are then used as training pairs (alongside some sampled helpful prompts) to fine-tune a new **SL-CAI model** (supervised learning constitutional AI).
 * A reinforcement learning phase then follows.
 * The SL-CAI model is used on a dataset of harmful prompts to generate two possible responses for each prompt.
 * The model is then asked to pick one of the responses based on a given constitutional principle to create a **preference dataset** (HF data can also be mixed in).
 * A **preference model** is trained on the preference dataset to give AI feedback.
 * The preference model is used to conduct a fine-tuning round of the SL-CAI model using RL with AI feedback (RLAIF) to build the final **RL-CAI model**.
* This process was shown to produce less harmful and evasive models (compared to RLHF) that give more explainable answers.


